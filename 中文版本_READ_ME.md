# 机器学习基础 《中文版本》

# 说明
1.数据准备：确保特征和标签文件路径正确，数据格式符合要求。<br>
2.数据预处理：根据数据特点，选择合适的预处理方法。<br>
3.模型选择：根据任务需求选择合适的监督学习模型，此处可以用[H2O]([url](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html))代替。<br>
4.参数调优(parameters tune)：使用交叉验证和网格搜索等方法优化模型参数。<br>
算法调参，指的是在机器学习或数据分析中，通过调整算法的参数，使损失函数（例如SSD中的loss）尽量小，来优化算法的性能。这通常涉及在训练数据集上运行算法，评估其表现（例如准确率、召回率等），然后根据评估结果调整参数，并重复这个过程，直到找到最佳参数组合，使算法在特定任务上达到最佳效果<br>

5.模型评估：使用准确率、精确率、召回率、F1分数、AUC等指标评估模型性能。<br>
6.结果解释：根据业务需求解释模型结果，提供可操作的建议。<br>
7.持续改进：根据模型表现和新数据，持续优化模型和数据处理流程。<br>


## 一、监督学习模型
### ‌1. 分类模型‌（输出用于离散标签预测）：
    K近邻（K-NN）‌，朴素贝叶斯（Naive Bayes）‌，逻辑回归（Logistic Regression）‌，
    决策树（Decision Tree），随机森林（Random Forest），支持向量机（SVM）‌‌，神经网络（Neural Networks）
### ‌2. 回归模型‌（输出用于连续值预测）：
    决策树回归（Decision Tree Regression）‌，随机森林回归（Random Forest Regression）‌，支持向量回归（SVR）‌，
    线性回归（Linear Regression）‌，多项式回归（Polynomial Regression）‌，岭回归（Ridge Regression）‌，Lasso 回归 (Lasso Regression)‌，弹性网回归（Elastic Net Regression）‌，
    # 梯度提升回归（Gradient Boosting Regression），XGBoost 回归（XGBoost Regression）‌，LightGBM 回归（LightGBM Regression‌），CatBoost 回归（CatBoost Regression）‌

## 二、无监督学习模型
###  ‌1. 聚类模型‌（数据分组）：
    K均值（K-Means）‌，层次聚类（Hierarchical Clustering）‌，DBSCAN（基于密度的聚类）‌
###  2. 降维模型‌（特征提取）：
    主成分分析（PCA）‌，t-SNE（可视化高维数据）‌，自编码器（Autoencoder）‌
### ‌ 3. 关联规则学习‌：
    Apriori算法（市场篮分析）
    
## 三、 集成模型
    Bagging (bootstrap aggregation)，Boosting， VotingClassifier，StackingClassifier，
    # 梯度提升回归（Gradient Boosting Regression），XGBoost 回归（XGBoost Regression）‌，LightGBM 回归（LightGBM Regression‌），CatBoost 回归（CatBoost Regression）‌



# 监督学习：分类模型
## 1. 《K近邻算法》
K近邻算法（K-Nearest Neighbors, KNN）是一种基于实例的监督学习算法，通过计算样本之间的距离来进行**分类**(回归很少用)。 <br>
对于一个新的样本，KNN 会找到训练集中与其距离最近的 K 个邻居，根据这些邻居的类别或数值来预测新样本的类别或数值。 <br>
KNN 的优点是模型简单，易于实现，能够处理多类别问题，缺点是计算复杂度较高，尤其是在大规模数据集上，且对噪声敏感。

## 2.决策树算法
决策树是一种树形结构的**分类和回归**模型，通过对数据进行递归划分，构建一棵树来表示决策过程。 <br>
每个节点表示一个特征的测试，每个分支表示测试结果的不同取值，叶节点表示最终的类别或数值预测。 <br>
决策树的优点是模型简单，易于解释，能够处理非线性关系，缺点是容易过拟合，尤其是在树深度较大时。


## 3.随机森林算法
随机森林是一种**集成学习方法**，通过构建多个决策树来进行**分类或回归**，并结合它们的预测结果来提高模型的准确性和鲁棒性。 <br>
每棵决策树在训练时使用随机选择的特征子集和样本子集，从而减少过拟合的风险。 <br>
随机森林的优点是能够处理高维数据，具有较强的抗噪能力，缺点是模型较复杂，难以解释。 <br>

## 4.支持向量机算法
支持向量机（SVM）是一种监督学习算法，主要用于**分类和回归**问题。<br>
SVM 的基本思想是找到一个最优超平面，将不同类别的样本分开，并最大化类别之间的间隔（margin）。<br>
对于线性不可分的数据，SVM 通过引入核函数（kernel function）将数据映射到高维空间，从而实现线性可分。<br>
SVM 的优点是能够处理高维数据，具有较强的泛化能力，缺点是对参数选择和核函数的选择较为敏感。<br>
<br>

## 5.《朴素贝叶斯》
朴素贝叶斯是一种基于贝叶斯定理的概率分类算法，假设特征之间相互独立。主要用于**分类问题**。<br>
它通过计算每个类别的先验概率和条件概率，结合贝叶斯定理来预测样本的类别。<br>
朴素贝叶斯适用于文本分类、垃圾邮件过滤等任务。<br>
朴素贝叶斯的优点是计算效率高，适用于大规模数据集，缺点是对特征独立性的假设较强，可能影响分类性能。<br>

## 6.《逻辑回归》
逻辑回归是一种广义_**_线性模型_**_，主要用于**二分类和多分类**问题。<br>
它通过对输入特征进行线性组合，并使用逻辑函数（sigmoid 函数）将结果映射到概率值，从而预测样本的类别。<br>
逻辑回归适用于信用评分、医疗诊断等任务。<br>
逻辑回归的优点是模型简单，易于解释，缺点是对线性可分数据效果较好，对非线性数据可能表现不佳。 <br>

## 7.神经网络
神经网络是一种模拟生物神经系统的计算模型，由多个层次的神经元（节点）组成。主要用于**分类和回归问题**。<br>
每个神经元接收输入信号，经过加权和偏置处理后，通过激活函数产生输出信号。<br>
神经网络通过调整权重和偏置来学习输入数据与输出标签之间的复杂关系。
神经网络的优点是能够处理复杂的_**_非线性关系_**_，适用于图像识别、自然语言处理等任务，缺点是训练时间较长，容易过拟合。<br>


# 监督学习：回归模型
## 11. 线性回归（Linear Regression）
线性回归是一种基本的**回归分析**方法，用于建模输入特征与连续输出变量之间的线性关系。<br>
它通过最小化预测值与实际值之间的均方误差来估计模型参数。<br>
线性回归适用于房价预测、销售预测等任务。<br>
线性回归的优点是模型简单，易于解释，计算效率高，缺点是对线性关系假设较强，可能无法捕捉复杂的_**_非线性关系_**_。<br>

## 12. 多项式回归（Polynomial Regression）
多项式回归是一种**扩展线性回归**的方法，通过引入多项式特征来捕捉输入特征与输出变量之间的_**_非线性关系_**_。<br>
它通过对输入特征进行多项式变换，然后使用线性回归模型进行拟合。多项式回归适用于复杂的非线性关系建模任务。<br>
多项式回归的优点是能够捕捉非线性关系，缺点是容易过拟合，尤其是在高阶多项式情况下。<br>

## 13. 岭回归（Ridge Regression）
岭回归是一种正则化的**线性回归方法**，通过在损失函数中引入 L2 正则化项来防止过拟合。<br>
它通过控制模型参数的大小，使得模型更加平滑，具有更好的泛化能力。<br>
岭回归适用于多重共线性问题和高维数据建模任务。<br>
岭回归的优点是能够处理多重共线性，提高模型的稳定性，缺点是需要选择合适的正则化参数。<br>

## 14. Lasso 回归 (Lasso Regression) 
Lasso 回归是一种正则化的**线性回归方法**，通过在损失函数中引入 L1 正则化项来防止过拟合。<br>
它不仅能够控制模型参数的大小，还能够实现特征选择，将不重要的特征的系数缩减为零。<br>
Lasso 回归适用于高维数据建模任务和特征选择。<br>
Lasso 回归的优点是能够处理高维数据，实现特征选择，提高模型的解释性，缺点是需要选择合适的正则化参数。<br>

## 15. 弹性网络回归（Elastic Net Regression‌）
弹性网络回归是一种结合了岭回归和 Lasso 回归的**正则化线性回归方法**，通过在损失函数中引入 L1 和 L2 正则化项来防止过拟合。<br>
它通过调整两个正则化参数的权重，能够同时实现特征选择和模型平滑。弹性网络回归适用于高维数据建模任务和特征选择。<br>
弹性网络回归的优点是能够处理高维数据，实现特征选择，提高模型的解释性，缺点是需要选择合适的正则化参数。<br>

## 16. 决策树回归 （Decision Tree Regression）
决策树回归是一种基于树结构的**回归**方法，通过递归地将数据划分为不同的区域，并在每个区域内使用简单的模型（如常数值）进行预测。<br>
决策树回归通过选择最佳的划分特征和划分点来最小化预测误差。决策树回归适用于非线性关系建模任务。<br>
决策树回归的优点是模型易于解释，能够处理**非线性关系**，缺点是容易过拟合，尤其是在树深度较大时。<br>

## 17. 随机森林回归 (Random Forest Regression)
随机森林回归是一种基于集成学习的回归方法，通过结合多个决策树的预测结果来提高模型的性能。适用于**非线性关系**建模任务。<br>
随机森林通过对训练数据进行有放回抽样，训练多个决策树，并将它们的预测结果进行平均来得到最终预测结果。<br>
随机森林回归的优点是能够处理非线性关系，提高模型的准确性和鲁棒性，缺点是模型较复杂，训练时间较长。 <br>

## 18. 支持向量回归（Support Vector Regression, SVR）
支持向量回归（SVR）是一种基于支持向量机的**回归**方法，通过在高维特征空间中寻找一个最优超平面来拟合输入特征与连续输出变量之间的关系。
SVR 通过引入 ε 不敏感损失函数，使得模型对小的误差不敏感，从而提高模型的泛化能力。SVR 适用于复杂的**非线性关系**建模任务。
SVR 的优点是能够处理非线性关系，具有良好的泛化能力，缺点是训练时间较长，参数选择较复杂。

# 集成模型
“三个臭皮匠，赛过诸葛亮” 
集成学习是一种通过结合多个基学习器的预测结果来提高模型性能的方法。主要用于**分类和回归问题**。<br>
常见的集成学习方法包括 Bagging（Bootstrap Aggregating）和 Boosting。<br>
Bagging 通过对训练数据进行有放回抽样，并行地训练多个基学习器（各自独立的同类模型），并将各个模型的输出结果按照投票或平均的策略（例如分类中可采用投票策略，回归中可采用平均策略）来得到最终预测结果。<br>
Boosting 则通过迭代地训练基学习器，串行地训练多个基学习器（前后依赖的同类模型），即后一个模型用来对前一个模型的输出结果进行纠正。每次关注前一轮分类错误的样本，从而逐步提高模型的准确性。<br>
Stacking 通过并行地训练多个基学习器（各自独立的不同类模型），然后通过训练一个元模型（meta-model）来将各个模型的输出结果进行结合。<br>
集成学习的优点是能够提高模型的准确性和鲁棒性，缺点是模型较复杂，训练时间较长。<br>
## 1. Bagging (bootstrap aggregation)
Bagging的策略是取所有基模型的“平均”值作为最终模型的输出结果，因此Bagging集成方法能够很好的降低模型高方差（过拟合）的情况。<br>
因此，通常来说，在使用Bagging集成方法的时候，可以尽量使得每个基模型都出现过拟合的现象。<br>
## 2. Boosting
Boosting集成方法经常被用于改善模型高偏差的情况（欠拟合现象）。<br>
常见的模型：XGBoost， AdaBoost 和 Gradient Tree Boosting (GBDT:Gradient Boosting Decision Tree 梯度提升树）<br>
## 3. VotingClassifier
VotingClassifier 是一种集成学习方法，通过结合多个不同的基学习器的预测结果来提高模型的性能，主要用于**分类问题**。<br>
它支持两种投票方式：硬投票（hard voting）和软投票（soft voting）。<br>
硬投票通过对基学习器的预测结果进行多数投票来决定最终的预测类别，而软投票则通过对基学习器的预测概率进行加权平均来决定最终的预测类别。<br>
VotingClassifier 的优点是能够结合多个不同类型的模型，利用它们的优势来提高整体性能，缺点是模型较复杂，训练时间较长。<br>
## 4. StackingClassifier
StackingClassifier 是一种集成学习方法，通过结合多个独立的不同类的基学习器的预测结果来提高模型的性能，主要用于**分类问题**。<br>
与 VotingClassifier 不同，StackingClassifier 使用一个元学习器（meta-learner）来学习基学习器的预测结果，从而生成最终的预测结果。<br>
元学习器通常是一个简单的模型，如逻辑回归或线性回归。<br>
StackingClassifier 的优点是能够结合多个不同类型的模型，利用它们的优势来提高整体性能，缺点是模型较复杂，训练时间较长。<br>
